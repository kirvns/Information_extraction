# -*- coding: utf-8 -*-
"""bewise_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R9lUcgcQPvSdpwNVqHwBqmqNNV3F9ahM
"""

!pip3 -qq install rusenttokenize
!pip3 -qq install pymorphy2
!pip3 -qq install nltk
!pip3 -qq install natasha

import pandas as pd
import pymorphy2
import string
import nltk
import re
from rusenttokenize import ru_sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk import word_tokenize
nltk.download('punkt')
from natasha import (
    Segmenter,
    MorphVocab,
    
    NewsEmbedding,
    NewsMorphTagger,
    NewsSyntaxParser,
    NewsNERTagger,
    
    PER,
    NamesExtractor,

    Doc
)

"""greeting, goodbye with cosine similarity"""

test_data = pd.read_csv("test_data.csv", encoding="utf-8")

all_txt_list = test_data['text'].tolist()

dlg_dict_mg = {}
for i in range(0,6):
  dlg_dict_mg['dlg'+str(i)+'_mg'] = (test_data.loc[(test_data["dlg_id"] == i)&(test_data["role"] == 'manager')])['text'].tolist()

dlg_dict_all = {}
for i in range(0,6):
  dlg_dict_all['dlg'+str(i)+'_all'] = (test_data.loc[test_data["dlg_id"] == i])['text'].tolist()

morph_analyzer = pymorphy2.MorphAnalyzer()

def preprocess_tokenize(text):
    text_preprocessed_tokenized = []      
    for sentence in ru_sent_tokenize(text):    
        clean_words = [word.strip(string.punctuation) for word in word_tokenize(text)] 
        clean_words = [word.lower() for word in clean_words]
        clean_lemmas = [morph_analyzer.parse(word)[0].normal_form for word in clean_words]
        text_preprocessed_tokenized.extend(clean_lemmas)
    return text_preprocessed_tokenized

intents = {"greeting": ["Привет", "Здравствуйте", "Добрый день", "Добрый вечер", "Здрасьте", "Здорова", "Добрый денёк", "День добрый", "Рад видеть", "Доброе утро", "Здравствуй", "Доброго времени", "Здравия желаю", "Салам", "Салют", "Бонжур", "Хелло", "Приветствую", "Добро пожаловать", "Доброго здоровья"], "goodbye": ["Пока", "До свидания", "Счастливо", "Всего доброго", "Всего хорошего", "До скорого", "Увидимся", "Спокойной ночи", "До встречи", "Свяжемся позже", "Прощай", "Встретимся", "До свиданья", "Удачи", "Всего наилучшего", "До связи", "Честь имею", "Разрешите откланяться", "Береги себя", "Хорошего дня", "Хорошего вечера", "Поговорим завтра"]}

train_phrases = []
for dialogue in dlg_dict_mg:
  for phrase in dlg_dict_mg[dialogue]:
      train_phrases.append(phrase)

for intent in intents:
  for phrase in intents[intent]:
      train_phrases.append(phrase)

vectorizer = CountVectorizer(tokenizer=preprocess_tokenize, ngram_range=(1,2))
vectorizer.fit_transform(train_phrases).toarray()

greetings, goodbyes = [],[]
for dialogue in dlg_dict_mg:
  for phrase in dlg_dict_mg[dialogue]:
      vec1 = vectorizer.transform([phrase])
      for intent in intents:
        for sentence in intents[intent]:
          vec2 = vectorizer.transform([sentence])
          similarity = cosine_similarity(vec1, vec2)
          if similarity > 0.3 and intent == 'greeting':
            if phrase not in greetings: 
              greetings.append(phrase)
          if similarity > 0.3 and intent == 'goodbye':
            if phrase not in goodbyes:
              goodbyes.append(phrase)

ethics_check = []
for text in all_txt_list:
  ethics = ''
  for sample in greetings:
    if sample == text:
      ethics = ethics+('greeting=True')
  for sample in goodbyes:
    if sample == text:
      ethics = ethics+('goodbye=True')
  ethics_check.append(ethics)

#unique values
greetings, goodbyes

test_data['ethics'] = ethics_check

"""greeting, goodbye with re"""

re_greetings, re_goodbyes = [],[]
for dialogue in dlg_dict_mg:
  for phrase in dlg_dict_mg[dialogue]:
    if re.findall(r'(.*дравств.*|.*ривет.*|.*обр.*день.*|.*обр.*вечер.*|.*обр.*вечер.*|.*обро.*пожалов.*)', phrase) != []:
      re_greetings.append(phrase)

for dialogue in dlg_dict_mg:
  for phrase in dlg_dict_mg[dialogue]:
    if re.findall(r'(.*свидания.*|.*пока.*|.*Пока*|.*сего.*доброго.*|.*сего.*хорошего.*|.*орошег.*дня.*|.*орошег.*вечер.*)', phrase) != []:
      re_goodbyes.append(phrase)

re_greetings

re_goodbyes

"""NER"""

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
syntax_parser = NewsSyntaxParser(emb)
ner_tagger = NewsNERTagger(emb)
names_extractor = NamesExtractor(morph_vocab)

def proper_names(text):
    propered_names = []      
    for sentence in text:    
      sentence_tokenized = word_tokenize(sentence)
      for word in sentence_tokenized:
        if morph_analyzer.parse(word)[0].score < 0.1 and 'INTJ' not in morph_analyzer.parse(word)[0].tag and len(word) > 3:
          propered_names.append(word)
        elif 'Name' in morph_analyzer.parse(word)[0].tag and len(word) > 3:
          propered_names.append(word)
    return propered_names

pr_names = proper_names(all_txt_list)
all_txt_propered = []
for sentence in all_txt_list:
  sentence = sentence.lower()
  for word in pr_names:
    if word in sentence:
      sentence = sentence.replace(word, word.capitalize())
  all_txt_propered.append(sentence)

ner_name = []
for text in all_txt_propered:
  text_span = ''
  doc = Doc(text)
  doc.segment(segmenter)
  doc.tag_morph(morph_tagger)
  doc.parse_syntax(syntax_parser)
  doc.tag_ner(ner_tagger)
  if doc.spans:
    for span in doc.spans:
      if span.type == 'PER' and ('компания '+span.text) not in text and ('зовут '+span.text) not in text and (span.text+'зовут ') not in text and ('это '+span.text) not in text:
        text_span = text_span+('person='+span.text+' ')
      if span.type == 'PER' and ('компания '+span.text) not in text and (('зовут '+span.text) in text or (span.text+'зовут ') in text or ('это '+span.text) in text):
        text_span = text_span+('self_name='+span.text+' ')
    ner_name.append(text_span)
  else:
    ner_name.append('')

test_data['ner_name'] = ner_name

ner_org = []
for text in all_txt_propered:
  text_span = ''
  doc = Doc(text)
  doc.segment(segmenter)
  doc.tag_morph(morph_tagger)
  doc.parse_syntax(syntax_parser)
  doc.tag_ner(ner_tagger)
  if doc.spans:
    for span in doc.spans:
      if span.type == 'PER' and ('компания '+span.text) in text:
        text_span = text_span+('organization='+span.text+' ')
      if span.type == 'ORG':
        text_span = text_span+('organization='+span.text+' ')
    ner_org.append(text_span)
  else:
    ner_org.append('')

test_data['ner_org'] = ner_org

test_data

test_data.to_csv('annotated_data.csv', index=False)

"""greeting"""

test_data.loc[(test_data['role'] == 'manager')&(test_data['ethics'] == 'greeting=True')]['text'].tolist()

"""goodbye"""

test_data.loc[(test_data['role'] == 'manager')&(test_data['ethics'] == 'goodbye=True')]['text'].tolist()

"""names"""

test_data.loc[(test_data['role'] == 'manager')&(test_data['ner_name'] != '')]['ner_name'].tolist()

"""organizations"""

test_data.loc[test_data['ner_org'] != '']['ner_org'].tolist()

"""checking the condition"""

for dialogue in range(0,6):
  if test_data.loc[(test_data["dlg_id"] == dialogue)&(test_data['ethics'] == 'greeting=True')]['text'].tolist() != []:
    if test_data.loc[(test_data["dlg_id"] == dialogue)&(test_data['ethics'] == 'goodbye=True')]['text'].tolist() != []:
      print('Требование о приветствии и прощании в диалоге '+ str(dialogue) + ' соблюдено')
    else:
      print('Требование о приветствии и прощании в диалоге '+ str(dialogue) + ' не соблюдено')  
  else:
    print('Требование о приветствии и прощании в диалоге '+ str(dialogue) + ' не соблюдено')